{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Task 1"]},{"cell_type":"markdown","metadata":{},"source":["### Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-29T20:38:49.461464Z","iopub.status.busy":"2024-01-29T20:38:49.461133Z","iopub.status.idle":"2024-01-29T20:38:53.966138Z","shell.execute_reply":"2024-01-29T20:38:53.965029Z","shell.execute_reply.started":"2024-01-29T20:38:49.461437Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils import data\n","from torch.nn import TransformerEncoder, TransformerEncoderLayer\n","\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import confusion_matrix\n","\n","import tqdm.autonotebook as tqdm\n","import dill\n","import math\n","import pickle"]},{"cell_type":"markdown","metadata":{},"source":["### Loading Pickles"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-29T20:38:53.968968Z","iopub.status.busy":"2024-01-29T20:38:53.968443Z","iopub.status.idle":"2024-01-29T20:38:53.983198Z","shell.execute_reply":"2024-01-29T20:38:53.981100Z","shell.execute_reply.started":"2024-01-29T20:38:53.968930Z"},"trusted":true},"outputs":[],"source":["pickle_folder_path = \"Pickles/\"\n","\n","import pickle\n","\n","def load_erc():\n","    with open(pickle_folder_path + \"idx2utt.pickle\",\"rb\") as f:\n","        idx2utt = pickle.load(f)\n","    with open(pickle_folder_path + \"utt2idx.pickle\",\"rb\") as f:\n","        utt2idx = pickle.load(f)\n","        \n","    with open(pickle_folder_path + \"idx2emo.pickle\",\"rb\") as f:\n","        idx2emo = pickle.load(f)\n","    with open(pickle_folder_path + \"emo2idx.pickle\",\"rb\") as f:\n","        emo2idx = pickle.load(f)\n","        \n","    with open(pickle_folder_path + \"idx2speaker.pickle\",\"rb\") as f:\n","        idx2speaker = pickle.load(f)\n","    with open(pickle_folder_path + \"speaker2idx.pickle\",\"rb\") as f:\n","        speaker2idx = pickle.load(f)\n","\n","    with open(pickle_folder_path + \"weight_matrix.pickle\",\"rb\") as f:\n","        weight_matrix = pickle.load(f)\n","\n","    with open(pickle_folder_path + \"train_data.pickle\",\"rb\") as f:\n","        my_dataset_train = pickle.load(f)\n","        \n","    with open(pickle_folder_path + \"test_data.pickle\",\"rb\") as f:\n","        my_dataset_test = pickle.load(f)\n","        \n","    with open(pickle_folder_path + \"final_speaker_info.pickle\",\"rb\") as f:\n","        final_speaker_info = pickle.load(f)\n","        \n","    with open(pickle_folder_path + \"final_speaker_dialogues.pickle\",\"rb\") as f:\n","        final_speaker_dialogues = pickle.load(f)\n","        \n","    with open(pickle_folder_path + \"final_speaker_emotions.pickle\",\"rb\") as f:\n","        final_speaker_emotions = pickle.load(f)\n","        \n","    with open(pickle_folder_path + \"final_speaker_indices.pickle\",\"rb\") as f:\n","        final_speaker_indices = pickle.load(f)\n","        \n","    with open(pickle_folder_path + \"final_utt_len.pickle\",\"rb\") as f:\n","        final_utt_len = pickle.load(f)\n","\n","    return idx2utt, utt2idx, idx2emo, emo2idx, idx2speaker,\\\n","        speaker2idx, weight_matrix, my_dataset_train, my_dataset_test,\\\n","        final_speaker_info, final_speaker_dialogues, final_speaker_emotions,\\\n","        final_speaker_indices, final_utt_len"]},{"cell_type":"markdown","metadata":{},"source":["### Layers"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-29T20:38:54.006193Z","iopub.status.busy":"2024-01-29T20:38:54.005801Z","iopub.status.idle":"2024-01-29T20:38:54.063539Z","shell.execute_reply":"2024-01-29T20:38:54.062464Z","shell.execute_reply.started":"2024-01-29T20:38:54.006155Z"},"trusted":true},"outputs":[],"source":["##Source: https://medium.com/@martinpella/how-to-use-pre-trained-word-embeddings-in-pytorch-71ca59249f76\n","def create_emb_layer(weights_matrix, utt2idx):\n","    num_embeddings, embedding_dim = weights_matrix.size()\n","    emb_layer = nn.Embedding(num_embeddings, embedding_dim, padding_idx=utt2idx[\"<pad>\"])\n","    emb_layer.load_state_dict({'weight': weights_matrix})\n","    emb_layer.weight.requires_grad = False\n","    return emb_layer, num_embeddings, embedding_dim\n","\n","class myRNN(nn.Module):\n","    def __init__(self,input_size,hidden_size,num_layers,dp=0,bd=False):\n","        super(myRNN,self).__init__()\n","        self.hidden_dim = hidden_size\n","        self.n_layers = num_layers\n","        self.RNN = nn.GRU(input_size = input_size,hidden_size=hidden_size,num_layers=num_layers,dropout=dp,batch_first=True,bidirectional=bd)\n","       \n","    def forward(self,x,h0=None):\n","        out,h = self.RNN(x,h0)\n","        return out,h\n","\n","class attention(nn.Module):\n","    def __init__(self,qembed_dim, kembed_dim=None, vembed_dim=None, hidden_dim=None, out_dim=None, dropout=0):\n","        super(attention, self).__init__()\n","        if kembed_dim is None:\n","            kembed_dim = qembed_dim\n","        if hidden_dim is None:\n","            hidden_dim = kembed_dim\n","        if out_dim is None:\n","            out_dim = kembed_dim\n","        if vembed_dim is None:\n","            vembed_dim = kembed_dim\n","            \n","        self.qembed_dim = qembed_dim\n","        self.kembed_dim = kembed_dim\n","        self.vembed_dim = vembed_dim\n","        \n","        self.hidden_dim = hidden_dim\n","        self.for_key = nn.Linear(kembed_dim,hidden_dim)\n","        self.for_query = nn.Linear(qembed_dim,hidden_dim)\n","        self.for_value = nn.Linear(vembed_dim,hidden_dim)\n","        self.normalise_factor = hidden_dim**(1/2)\n","    \n","    def mask_score(self,s,m):\n","        for i in range(s.size()[0]):\n","            for j in range(s.size()[1]):\n","                for k in range(s.size()[2]):\n","                    if m[i][j][k] == 0:\n","                        s[i][j][k] = float('-inf')   #So that after softmax, 0 weight is given to it\n","        return s\n","    \n","    def forward(self,key,query,mask=None):\n","        if len(query.shape) == 1:\n","            query = torch.unsqueeze(query, dim=0)\n","        if len(key.shape) == 1:\n","            key = torch.unsqueeze(key, dim=0)\n","            \n","        if len(query.shape) == 2:\n","            query = torch.unsqueeze(query, dim=1)\n","        if len(key.shape) == 2:\n","            key = torch.unsqueeze(key, dim=1)\n","            \n","        new_query = self.for_query(query)\n","        new_key = self.for_key(key)\n","        new_value = self.for_value(key)\n","        \n","        score = torch.bmm(new_query,new_key.permute(0,2,1))/self.normalise_factor\n","        \n","        if mask != None:\n","            score = self.mask_score(score,mask)\n","            \n","        score = F.softmax(score,-1)\n","        score.data[score!=score] = 0         #removing nan values\n","        \n","        output = torch.bmm(score,new_value)\n","        return output,score\n","\n","class interact(nn.Module):\n","    def __init__(self,hidden_dim,weight_matrix,utt2idx):\n","        super(interact, self).__init__()\n","        self.hidden_size = hidden_dim\n","\n","        self.embedding, num_embeddings, embedding_dim = create_emb_layer(weight_matrix,utt2idx)\n","        self.rnnD = myRNN(embedding_dim, hidden_dim,1)   #Dialogue\n","        self.drop1 = nn.Dropout()\n","        \n","        self.rnnG = myRNN(embedding_dim*3, hidden_dim,1)   #Global level\n","        self.drop2 = nn.Dropout()\n","        \n","        self.attn = attention(embedding_dim)\n","        \n","        self.rnnS = myRNN(embedding_dim*2, embedding_dim*2,1)   #Speaker representation\n","        self.drop3 = nn.Dropout()\n","\n","    def forward(self, chat_ids, speaker_info, sp_dialogues, sp_ind, inputs):\n","        whole_dialogue_indices = inputs\n","        \n","        bert_embs = self.embedding(whole_dialogue_indices)\n","               \n","        dialogue, h1 = self.rnnD(bert_embs)    #Get global level representation\n","        dialogue = self.drop1(dialogue)\n","\n","        device = inputs.device\n","        \n","        fop = torch.zeros((dialogue.size()[0],dialogue.size()[1],dialogue.size()[2])).to(device)\n","        fop2 = torch.zeros((dialogue.size()[0],dialogue.size()[1],dialogue.size()[2]*3)).to(device)\n","        op = torch.zeros((dialogue.size()[0],dialogue.size()[1],dialogue.size()[2])).to(device)\n","        spop = torch.zeros((dialogue.size()[0],dialogue.size()[1],dialogue.size()[2]*2)).to(device)\n","                    \n","        h0 = torch.randn(1, 1, self.hidden_size*2).to(device)\n","        d_h = torch.randn(1, 1, self.hidden_size).to(device)\n","        attn_h = torch.randn(1, 1, self.hidden_size).to(device)\n","        \n","        for b in range(dialogue.size()[0]):\n","            d_id = chat_ids[b]\n","            speaker_hidden_states = {}\n","            for s in range(dialogue.size()[1]):\n","                fop = op.clone()\n","                \n","                current_utt = dialogue[b][s]\n","                \n","                current_speaker = speaker_info[d_id][s]\n","                \n","                if current_speaker not in speaker_hidden_states:\n","                    speaker_hidden_states[current_speaker] = h0\n","                \n","                h = speaker_hidden_states[current_speaker]\n","                current_utt_emb = torch.unsqueeze(torch.unsqueeze(current_utt,0),0)\n","                \n","                key = fop[b][:s+1].clone()\n","                key = torch.unsqueeze(key,0)\n","                \n","                if s == 0:\n","                    tmp = torch.cat([attn_h,current_utt_emb],-1).to(device)\n","                    spop[b][s], h_new = self.rnnS(tmp,h)\n","                else:\n","                    query = current_utt_emb\n","                    attn_op,_ = self.attn(key,query)\n","                    \n","                    tmp = torch.cat([attn_op,current_utt_emb],-1).to(device)\n","                    spop[b][s], h_new = self.rnnS(tmp,h)\n","                \n","                spop[b][s] = spop[b][s].add(tmp)        # Residual Connection        \n","                speaker_hidden_states[current_speaker] = h_new\n","                \n","                fop2[b][s] = torch.cat([spop[b][s],dialogue[b][s]],-1)\n","                tmp = torch.unsqueeze(torch.unsqueeze(fop2[b][s].clone(),0),0)\n","                op[b][s],d_h = self.rnnG(tmp,d_h)\n","\n","        return op,spop\n","    \n","class fc_e(nn.Module):\n","    def __init__(self,inp_dim,op_dim):\n","        super(fc_e,self).__init__()\n","        self.linear1 = nn.Linear(inp_dim,int(inp_dim/2))\n","        self.drop1 = nn.Dropout()\n","        \n","        self.linear2 = nn.Linear(int(inp_dim/2),int(inp_dim/4))\n","        self.drop2 = nn.Dropout(0.6)\n","        \n","        self.linear3 = nn.Linear(int(inp_dim/4),op_dim)\n","        self.drop3 = nn.Dropout(0.7)\n","    def forward(self,x):\n","        ip = x.float()\n","    \n","        op = self.linear1(ip)\n","        op = self.drop1(op)\n","        \n","        op = self.linear2(op)\n","        op = self.drop2(op)\n","        \n","        op = self.linear3(op)\n","        op = self.drop3(op)\n","        \n","        return op\n","\n","class fc_t(nn.Module):\n","    def __init__(self,inp_dim,op_dim):\n","        super(fc_t,self).__init__()\n","        self.linear1 = nn.Linear(inp_dim,inp_dim)\n","        self.drop1 = nn.Dropout(0.7)\n","        \n","        self.linear2 = nn.Linear(inp_dim,inp_dim)\n","        self.drop2 = nn.Dropout(0.7)\n","        \n","        self.linear3 = nn.Linear(inp_dim,int(inp_dim/2))\n","        self.drop3 = nn.Dropout(0.7)\n","        \n","        self.linear4 = nn.Linear(int(inp_dim/2),int(inp_dim/4))\n","        self.drop4 = nn.Dropout(0.7)\n","        \n","        self.linear5 = nn.Linear(int(inp_dim/4),op_dim)\n","        self.drop5 = nn.Dropout(0.7)\n","    def forward(self,x):\n","        ip = x.float()\n","    \n","        op = self.linear1(ip)\n","        op = self.drop1(op)\n","        \n","        op = self.linear2(ip)\n","        op = self.drop2(op)\n","        \n","        op = self.linear3(ip)\n","        op = self.drop3(op)\n","        \n","        op = self.linear4(op)\n","        op = self.drop4(op)\n","        \n","        op = self.linear5(op)\n","        op = self.drop5(op)\n","        \n","        return op\n","    \n","class maskedattn(nn.Module):\n","    def __init__(self,batch_size, s_len, emb_size):\n","        super(maskedattn,self).__init__()\n","        self.b_len = batch_size\n","        self.s_len = s_len\n","        self.emb_size = emb_size\n","        self.attn = attention(emb_size*2, kembed_dim=emb_size, out_dim=emb_size)\n","    \n","    def create_mask(self,n):\n","        mask = torch.zeros((1, self.s_len, self.emb_size), dtype=torch.uint8)\n","        mask[:n+1] = torch.ones((self.emb_size), dtype=torch.uint8)\n","        mask = mask.repeat(self.b_len,1,1)\n","        return mask\n","        \n","    def forward(self,key,query):\n","        device = key.device\n","\n","        ops = torch.zeros([key.size()[0],key.size()[1], key.size()[2]], dtype=torch.float32).to(device)\n","        for i in range(key.size()[1]):\n","          mask = self.create_mask(i)\n","          op,_ = self.attn(key,query,mask=mask)\n","          for b in range(op.size()[0]):\n","            ops[b][i] = op[b][i]\n","        return ops\n","    \n","class memnet(nn.Module):\n","  def __init__(self,num_hops,hidden_size,batch_size,seq_len):\n","    super(memnet,self).__init__()\n","    self.num_hops = num_hops\n","    self.rnn = myRNN(hidden_size, hidden_size, 1)\n","    self.masked_attention = maskedattn(batch_size,seq_len,hidden_size)\n","  \n","  def forward(self,globl,spl):\n","    X = globl\n","    for hop in range(self.num_hops):\n","      dialogue,h = self.rnn(X)\n","      X = self.masked_attention(dialogue,spl)\n","    return X\n","\n","class pool(nn.Module):\n","    def __init__(self,mode=\"mean\"):\n","        super(pool,self).__init__()\n","        self.mode = mode\n","    def forward(self,x):\n","        device = x.device\n","        op = torch.zeros((x.size()[0],x.size()[1],x.size()[2])).to(device)\n","        for b in range(x.size()[0]):\n","            this_tensor = []\n","            for s in range(x.size()[1]):\n","                this_tensor.append(x[b][s])\n","                if self.mode == \"mean\":\n","                    op[b][s] = torch.mean(torch.stack(this_tensor),0)\n","                elif self.mode == \"max\":\n","                    op[b][s],_ = torch.max(torch.stack(this_tensor),0)\n","                elif self.mode == \"sum\":\n","                    op[b][s] = torch.sum(torch.stack(this_tensor),0)\n","                else:\n","                    print(\"Error: Mode can be either mean or max only\")\n","        return op\n","\n","class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, dropout=0.1, max_len=5000):\n","        super(PositionalEncoding, self).__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        pe = pe.unsqueeze(0).transpose(0, 1)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        x = x + self.pe[:x.size(0), :]\n","        return self.dropout(x)"]},{"cell_type":"markdown","metadata":{},"source":["### Model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-29T20:38:54.065415Z","iopub.status.busy":"2024-01-29T20:38:54.064997Z","iopub.status.idle":"2024-01-29T20:38:54.079470Z","shell.execute_reply":"2024-01-29T20:38:54.078364Z","shell.execute_reply.started":"2024-01-29T20:38:54.065381Z"},"trusted":true},"outputs":[],"source":["top_speaker_names = [\"maya\", \"indu\", \"sahil\", \"monisha\", \"rosesh\", \"madhusudhan\"]\n","NUM_SPK = len(top_speaker_names)\n","def get_spk_embedding(spk_original_ix):\n","    name = idx2speaker[spk_original_ix]\n","    if name in top_speaker_names: \n","        vec = torch.nn.functional.one_hot(torch.tensor(top_speaker_names.index(name), device=device), num_classes=NUM_SPK)\n","    else:\n","        vec = torch.zeros(NUM_SPK, device=device)\n","    return vec\n","\n","class ERC_MMN(nn.Module):\n","    def __init__(self,hidden_size,weight_matrix,utt2idx,batch_size,seq_len):\n","        super(ERC_MMN,self).__init__()\n","        self.ia = interact(hidden_size,weight_matrix,utt2idx)\n","        self.mn = memnet(4,hidden_size,batch_size,seq_len)\n","        self.pool = pool()\n","        \n","        self.rnn_c = myRNN(hidden_size*3,hidden_size*2,1)\n","        \n","        self.rnn_e = myRNN(hidden_size*2,hidden_size*2,1)\n","                \n","        self.linear1 = fc_e(hidden_size*2,8)\n","\n","    def forward(self,c_ids,speaker_info,sp_dialogues,sp_em,sp_ind,x1,mode=\"train\"):\n","        glob, splvl = self.ia(c_ids,speaker_info,sp_dialogues,sp_ind,x1)\n","\n","        op = self.mn(glob,splvl)\n","        op = self.pool(op)\n","\n","        op = torch.cat([splvl,op],dim=2)\n","\n","        rnn_c_op,_ = self.rnn_c(op)\n","\n","        rnn_e_op,_ = self.rnn_e(rnn_c_op)\n","        fip = rnn_e_op.add(rnn_c_op)      # Residual Connection\n","        fop1 = self.linear1(fip)\n","\n","        return fip,fop1"]},{"cell_type":"markdown","metadata":{},"source":["### Updating utterance embedding with speaker-representation"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Set device\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Load Pickles\n","idx2utt, utt2idx, idx2emo, emo2idx, idx2speaker,\\\n","    speaker2idx, weight_matrix, my_dataset_train, my_dataset_test,\\\n","    final_speaker_info, final_speaker_dialogues, final_speaker_emotions,\\\n","    final_speaker_indices, final_utt_len = load_erc()\n","    \n","weight_matrix = weight_matrix.to(device)\n","\n","# Updation of weight matrix\n","utt_ix_set = set()\n","n = len(weight_matrix)\n","d = len(weight_matrix[1])\n","\n","new_weight_matrix = torch.zeros([n,d+NUM_SPK])\n","for ix1, sample in enumerate(my_dataset_train):\n","    for ix2, utt_ix in enumerate(sample[1]):\n","            ix_u = int(utt_ix)\n","            spk_ix = final_speaker_info[ix1][ix2]\n","            new_weight_matrix[ix_u] = torch.cat([weight_matrix[ix_u], get_spk_embedding(spk_ix)])\n","            utt_ix_set.add(ix_u)\n","\n","for ix1, sample in enumerate(my_dataset_test):\n","    for ix2, utt_ix in enumerate(sample[1]):\n","            ix_u = int(utt_ix)\n","            spk_ix = final_speaker_info[ix1][ix2]\n","            new_weight_matrix[ix_u] = torch.cat([weight_matrix[ix_u], get_spk_embedding(spk_ix)])\n","            utt_ix_set.add(ix_u)\n","            \n","weight_matrix = new_weight_matrix"]},{"cell_type":"markdown","metadata":{},"source":["### Training and Validation"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-29T20:38:54.081086Z","iopub.status.busy":"2024-01-29T20:38:54.080785Z","iopub.status.idle":"2024-01-29T20:46:25.524052Z","shell.execute_reply":"2024-01-29T20:46:25.522478Z","shell.execute_reply.started":"2024-01-29T20:38:54.081053Z"},"trusted":true},"outputs":[],"source":["def get_train_test_loader(bs):\n","    my_dataset_train_new = []\n","    for a,b,c,d,e in my_dataset_train:\n","        b = b.to(device)\n","        c = c.to(device)\n","        my_dataset_train_new.append([a,b,c,d,e])\n","        \n","    my_dataset_test_new = []  \n","    for a,b,c,d,e in my_dataset_test:\n","        b = b.to(device)\n","        c = c.to(device)\n","        my_dataset_test_new.append([a,b,c,d,e])\n","        \n","    train_data_iter = data.DataLoader(my_dataset_train_new,batch_size=bs)\n","    test_data_iter = data.DataLoader(my_dataset_test_new,batch_size=bs)\n","    \n","    return train_data_iter, test_data_iter\n","\n","\n","def train(model, train_data_loader, epochs):\n","    class_weights1 = torch.FloatTensor(weights1).to(device)\n","    criterion1 = nn.CrossEntropyLoss(weight=class_weights1,reduction='none').to(device)\n","\n","    optimizer = torch.optim.Adam(model.parameters(),lr=1e-4,weight_decay=1e-5)    \n","    max_f1_1 = 0\n","    \n","    for epoch in tqdm.tqdm(range(epochs)):\n","        print(\"\\n\\n-------Epoch {}-------\\n\\n\".format(epoch+1))\n","        model.train()\n","        \n","        avg_loss = 0\n","        \n","        y_true1 = []\n","        y_pred1 = []\n","            \n","        for i_batch, sample_batched in tqdm.tqdm(enumerate(train_data_loader), total=len(train_data_loader)):\n","            dialogue_ids = sample_batched[0].tolist()            \n","            inputs = sample_batched[1]\n","            targets1 = sample_batched[2]\n","                 \n","            optimizer.zero_grad()\n","            \n","            _, outputs = model(dialogue_ids, final_speaker_info, final_speaker_dialogues, final_speaker_emotions, final_speaker_indices, inputs)\n","            \n","            loss = 0\n","            for b in range(outputs.size()[0]):\n","              loss1 = 0\n","              for s in range(final_utt_len[dialogue_ids[b]]):\n","                pred1 = torch.unsqueeze(outputs[b][s],dim=0).to(device)\n","                truth1 = torch.LongTensor([targets1[b][s].item()]).to(device)\n","\n","                pred_emo = torch.argmax(F.softmax(pred1,-1),-1)\n","                \n","                y_pred1.append(pred_emo.item())\n","                y_true1.append(truth1.item())\n","\n","                loss1 += criterion1(pred1,truth1)\n","\n","              loss1 /= final_utt_len[dialogue_ids[b]]\n","              loss += loss1\n","\n","            avg_loss += loss\n","\n","            loss.backward()            \n","            optimizer.step()\n","            \n","        avg_loss /= len(train_data_loader)\n","        print(\"Average Loss = \",avg_loss)        \n","        \n","        if epoch%10==0:\n","            f1_1,v_loss = validate(model,False,epoch)        \n","            f1_1,v_loss = validate(model,True,epoch)\n","            print(f\"Saving model at epoch {epoch} with score {f1_1}\")\n","            with open(\"spk_model\" + str(epoch), \"wb\") as dill_file:\n","                dill.dump(model, dill_file)\n","\n","    return model\n","\n","\n","def validate(model, is_test ,epoch):\n","    print(\"\\n\\n***VALIDATION ({})***\\n\\n\".format(epoch))\n","    class_weights1 = torch.FloatTensor(weights1).to(device)\n","    criterion1 = nn.CrossEntropyLoss(weight=class_weights1,reduction='none')\n","    \n","    model.eval()\n","\n","    with torch.no_grad():\n","      avg_loss = 0\n","        \n","      y_true1 = []\n","      y_pred1 = []\n","        \n","      data_loader = data_iter_test\n","      if is_test==False:\n","            data_loader = data_iter_train\n","\n","      for i_batch, sample_batched in tqdm.tqdm(enumerate(data_loader), total=len(data_loader)):\n","            dialogue_ids = sample_batched[0].tolist()\n","            if is_test==False:\n","                val_train_cnt = 0\n","            else:\n","                val_train_cnt = train_cnt\n","            dialogue_ids = [val_train_cnt+d for d in dialogue_ids]\n","            inputs = sample_batched[1]\n","            targets1 = sample_batched[2]\n","\n","            _, outputs = model(dialogue_ids, final_speaker_info, final_speaker_dialogues, final_speaker_emotions, final_speaker_indices, inputs, mode=\"valid\")\n","            \n","            loss = 0\n","            for b in range(outputs.size()[0]):\n","              loss1 = 0\n","              for s in range(final_utt_len[dialogue_ids[b]]):\n","                pred1 = torch.unsqueeze(outputs[b][s],dim=0).to(device)\n","                truth1 = torch.LongTensor([targets1[b][s].item()]).to(device)\n","\n","                pred_emo = torch.argmax(F.softmax(pred1,-1),-1)\n","                \n","                y_pred1.append(pred_emo.item())\n","                y_true1.append(truth1.item())\n","\n","                loss1 += criterion1(pred1,truth1)\n","\n","              loss1 /= final_utt_len[dialogue_ids[b]]\n","              loss += loss1\n","\n","            avg_loss += loss\n","\n","      avg_loss /= len(data_loader)\n","\n","      class_report = classification_report(y_true1,y_pred1)\n","      conf_mat1 = confusion_matrix(y_true1,y_pred1)\n","\n","      print(class_report)\n","      print(\"Confusion Matrix: \\n\",conf_mat1)\n","    \n","      wtd_f1 = f1_score(y_true1,y_pred1,average=\"weighted\")\n","      return wtd_f1, avg_loss"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Parameters\n","batch_size = 64\n","seq_len = 15\n","emb_size = 768+6\n","hidden_size = 768+6\n","batch_first = True\n","train_cnt = len(my_dataset_train)  \n","weights1 = [1/math.sqrt(125.0),1/math.sqrt(1595.0),1/math.sqrt(441.0),1/math.sqrt(817.0),1/math.sqrt(513.0),1/math.sqrt(3908.0),1/math.sqrt(556.0),1/math.sqrt(542.0)]\n","data_iter_train, data_iter_test = get_train_test_loader(batch_size)\n","\n","# Training\n","model = ERC_MMN(hidden_size,weight_matrix,utt2idx,batch_size,seq_len).to(device)\n","model = train(model, data_iter_train, epochs = 300)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30636,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
