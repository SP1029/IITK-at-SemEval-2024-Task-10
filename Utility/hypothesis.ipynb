{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code to apply the hypothesis to the predictions of the model\n",
    "- Update the file initials to proceed\n",
    "- If you want to apply the hypothesis to a file containing single task predictions then update the Index constants accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "\n",
    "import json\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File Initials\n",
    "\n",
    "initial_predictions = \"prediction.txt\"\n",
    "file_initials_task2 = \"MaSaC_test_efr\"\n",
    "file_initials_task3 = \"MELD_test_efr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants denoting line number in final file for each task\n",
    "\n",
    "IX1_BEGIN = 0\n",
    "IX1_END = 1579\n",
    "IX2_BEGIN = 1580\n",
    "IX2_END = 9269\n",
    "IX3_BEGIN = 9270\n",
    "IX3_END = 17911"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Json Files\n",
    "\n",
    "file_name_task2 = \"./\" + file_initials_task2 + \".json\"\n",
    "with open(file_name_task2, 'r') as file:\n",
    "    dataset_task2 = json.load(file)\n",
    "\n",
    "file_name_task3 = \"./\" + file_initials_task3 + \".json\"\n",
    "with open(file_name_task3, 'r') as file:\n",
    "    dataset_task3 = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data of the prediction file\n",
    "\n",
    "f = open(initial_predictions, 'r')\n",
    "soln = []\n",
    "# Task 1 - MaSaC ERC\n",
    "for i in range(IX1_BEGIN, IX1_END+1):\n",
    "    x = f.readline()\n",
    "    soln.append(x[:-1])\n",
    "\n",
    "# Task 2 - MaSaC EFR\n",
    "for i in range(IX2_BEGIN, IX2_END+1):\n",
    "    x = f.readline()\n",
    "    soln.append(x[:-1])\n",
    "\n",
    "# Task 3 - MELD EFR\n",
    "for i in range(IX3_BEGIN, IX3_END+1):\n",
    "    x = f.readline()\n",
    "    soln.append(x[:-1])\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lengths(dataset):\n",
    "\n",
    "    # Reading the data\n",
    "\n",
    "    init_len = []\n",
    "\n",
    "    speakers_list = []\n",
    "    utterances_list = []\n",
    "\n",
    "    for i in range(len(dataset)):\n",
    "        speakers_list.append(dataset[i]['speakers'])\n",
    "        utterances_list.append(dataset[i]['utterances'])\n",
    "        init_len.append(len(dataset[i]['emotions']))\n",
    "\n",
    "    N = len(dataset)\n",
    "\n",
    "    # Calculate the new length of each converation according to the hypothesis\n",
    "\n",
    "    new_len = []\n",
    "\n",
    "    for i in range(len(dataset)):\n",
    "        count_utt = init_len[i]\n",
    "        target_speaker = speakers_list[i][count_utt-1]\n",
    "\n",
    "    for j in range(count_utt):\n",
    "        ix = count_utt - 1 - j\n",
    "\n",
    "        if ix != count_utt - 1:\n",
    "\n",
    "            if speakers_list[i][ix] == target_speaker:\n",
    "                new_len.append(min(5, j+1))\n",
    "                break\n",
    "\n",
    "    return init_len, new_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising the distribution of lengths\n",
    "\n",
    "def print_distr(len):\n",
    "\n",
    "    counter = Counter(len)\n",
    "    print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the hypothesis\n",
    "\n",
    "def apply_hypothesis(IX_BEGIN, IX_END, dataset, soln):\n",
    "\n",
    "    # Get the lengths\n",
    "    init_len, new_len = get_lengths(dataset)\n",
    "\n",
    "    # Print the distributions\n",
    "    print(\"Initial Length\")\n",
    "    print_distr(init_len)\n",
    "    print(\"New Length\")\n",
    "    print_distr(new_len)\n",
    "\n",
    "    # Mask the predictions to 0.0\n",
    "    i = 0\n",
    "    changes = 0\n",
    "    ix = IX_BEGIN\n",
    "\n",
    "    while ix < IX_END+1:\n",
    "\n",
    "        count_utt = init_len[i]\n",
    "        count_pred = new_len[i]\n",
    "        init_zeros = count_utt - count_pred\n",
    "\n",
    "        for j in range(init_zeros):\n",
    "            if soln[ix] == '1.0':\n",
    "                changes += 1\n",
    "            soln[ix] = '0.0'\n",
    "            ix += 1\n",
    "\n",
    "        ix += count_utt-init_zeros\n",
    "        i += 1\n",
    "\n",
    "    print(changes, \" predictions were converted from 1.0 to 0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying Hypothesis\n",
    "\n",
    "apply_hypothesis(IX2_BEGIN, IX2_END, dataset_task2, soln)\n",
    "apply_hypothesis(IX2_BEGIN, IX2_END, dataset_task3, soln)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the updated predictions into a new file\n",
    "\n",
    "f = open(\"new_predictions.txt\", 'w+')\n",
    "\n",
    "# Task 1 - MaSaC ERC\n",
    "for i in range(IX1_BEGIN, IX1_END+1):\n",
    "    f.write(str(soln[i])+'\\n')\n",
    "    print(soln[i])\n",
    "\n",
    "# Task 2 - MaSaC EFR\n",
    "for i in range(IX2_BEGIN, IX2_END+1):\n",
    "    f.write(str(soln[i])+'\\n')\n",
    "    print(soln[i])\n",
    "\n",
    "# Task 3 - MELD EFR\n",
    "for i in range(IX3_BEGIN, IX3_END+1):\n",
    "    f.write(str(soln[i])+'\\n')\n",
    "\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
